{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6583e1fec94ebebd1f1b2449cf9480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1717213352991_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-72-212.ec2.internal:20888/proxy/application_1717213352991_0001/\" class=\"emr-proxy-link\" emr-resource=\"j-3GYKK8J0F4NNB\n",
       "\" application-id=\"application_1717213352991_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-72-180.ec2.internal:8042/node/containerlogs/container_1717213352991_0001_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=livy-session-0>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3cad7123-c888-48a6-81ae-91543e02d59c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Lab 8: Spark Streaming For Log Processing\n",
    "\n",
    "This is a simple exercise in log processing.  The log files come from various servers at various time points.\n",
    "Each record in a log file is of the form ```serverID,severity,timestamp```, where  \n",
    "    - `serverID` is a string unique to the server  \n",
    "    - `severity` is a value of 2 (referred to as `SEV2` that represents no error, just a service call),  1 (referred to as `SEV1` that represents a minor error), or 0 (referred to as `SEV0` that represents a fatal/severe error)    \n",
    "    - `timestamp` is an integer starting at 1 (bigger numbers mean later)  \n",
    "\n",
    "For this lab, the four log files (on Canvas and Teams) will be \"delivered\" to Spark by being placed in a \"live data\" S3 bucket, for example `s3://hankssteven-week9/data/LogDataLive/`.\n",
    "\n",
    "There are two servers in the log files, `s1` and `s2`, and the log records range from `t1` to `t10`.  \n",
    "The files are delivered with one file per server for five time units. For example, the file `s115.csv` has records for server `s1` for times `t1` to `t5`.\n",
    "\n",
    "You want to process these new records incrementally, and are interested in these two \"reports\":\n",
    "\n",
    "1. The *volume report*: reports the number of `SEV2` events divided by the number of time units for each server. The number of time units for our purposes is `max(timestamp) - min(timestamp) + 1`. This volume report will not be cumulative, i.e., every time new log data comes in, the mapping from the server to `SEV2` events is updated  \n",
    "2. The *SEV0 log*: this is a sequence of records of the form ```serverID timestamp``` recording the timestamp of a `SEV0` event reported by a server. This report grows over time, i.e., each time a new log file is processed, new records are appended to the end.\n",
    "\n",
    "Your final reports will be produced by two streaming queries:\n",
    "1. One that *modifies* the `SEV2` volume report, which is stored in memory and will be queried in this Spark notebook\n",
    "2. One that *appends* to the `SEV0` log report, which will be stored as CSV records on S3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Following Buckets and Folders to Hold your Streaming Input and Output\n",
    "\n",
    "1. Create a bucket *yourname-week9* in S3.  Remember the convention that *yourname* is your SeattleU username.\n",
    "2. Create a folder *data* within that bucket\n",
    "3. Create a folder *LogData* within the data folder\n",
    "4. Upload the four log files from the Teams folder to your *LogData* folder\n",
    "5. Create an empty folder *LogDataLive* (for simulating log streams)\n",
    "6. Create an empty folder *StreamingOutput* (for saving your results)\n",
    "\n",
    "The cell below will list your S3 folders.  Replace the bucket name with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE LogData/\n",
      "                           PRE LogDataLive/\n",
      "                           PRE StreamingOutput/\n",
      "2024-06-01 03:52:49          0 \n",
      "---\n",
      "2024-06-01 03:53:50          0 \n",
      "2024-06-01 03:54:31      16000 s115.csv\n",
      "2024-06-01 03:54:30        820 s1610.csv\n",
      "2024-06-01 03:54:32      40000 s215.csv\n",
      "2024-06-01 03:54:30       8210 s2610.csv\n",
      "---\n",
      "2024-06-01 03:57:09          0 \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 ls s3://mbhavanagarwala-week9/data/\n",
    "echo \"---\"\n",
    "aws s3 ls s3://mbhavanagarwala-week9/data/LogData/\n",
    "echo \"---\"\n",
    "aws s3 ls s3://mbhavanagarwala-week9/data/LogDataLive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34ff72b0-61d4-4e92-b478-c0c84dc9c375",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df0619a573a434bbe38dc7f196b3a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the schema for the log files based on the above description of the data \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "logSchema = StructType([\n",
    "    StructField(\"serverID\", StringType(), True),\n",
    "    StructField(\"severity\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a513f133-14fd-4e46-bba8-855f401aadc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0670a931d7d6498aa8d70de2df05a36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the streaming DataFrame (readStream) on your log directory, using the schema you just created\n",
    "streamingLogData = spark.readStream\\\n",
    ".schema(logSchema)\\\n",
    ".option(\"maxFilesPerTrigger\",1)\\\n",
    ".csv(\"s3://mbhavanagarwala-week9/data/LogDataLive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Get the SEV2 volume report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e4dd595-b686-4ba0-9baf-3326c63995f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0c25f69120486585a9b1be110676d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the data frame you just created to create another data frame with the \n",
    "# sev2 volume report.  It should have columns 'serverID' and 'avgVolume'\n",
    "\n",
    "from pyspark.sql.functions import col, count, min, max, expr\n",
    "\n",
    "\n",
    "# Calculate the SEV2 volume report\n",
    "volumeReportDataFrame = streamingLogData.filter(col(\"severity\") == 2).groupBy(\"serverID\") \\\n",
    "    .agg(\n",
    "        count(\"severity\").alias(\"SEV2_count\"),\n",
    "        min(\"timestamp\").alias(\"min_timestamp\"),\n",
    "        max(\"timestamp\").alias(\"max_timestamp\")\n",
    "    ) \\\n",
    "    .withColumn(\"time_units\", col(\"max_timestamp\") - col(\"min_timestamp\") + 1) \\\n",
    "    .withColumn(\"avgVolume\", col(\"SEV2_count\") / col(\"time_units\")) \\\n",
    "    .select(\"serverID\", \"avgVolume\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad420b2b-d748-4c5b-b568-5369c44c9396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637af8b4706345f1a7a4ed9156e0a9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and start a query (writeStream) that generates the sev2 report;  it is an in-memory sink.\n",
    "volumeReportQuery = volumeReportDataFrame.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"volumeReport\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a44bb60-19c9-4f50-9278-641fb4806ca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b8d5b41cda4e6e915d65a7a90562b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|serverID|avgVolume|\n",
      "+--------+---------+\n",
      "+--------+---------+"
     ]
    }
   ],
   "source": [
    "# Write a (very simple) spark SQL query to show the contents of your sev2 report (volumeReportQuery). It should initially be empty\n",
    "volumeReportDF = spark.sql(\"SELECT * FROM volumeReport\")\n",
    "volumeReportDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the files s115 and s215 from LogData to LogDataLive.  Correct the bucket name below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6cd1418b-7ed3-4713-a5b7-7f8162fbfa2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://mbhavanagarwala-week9/data/LogData/s115.csv to s3://mbhavanagarwala-week9/data/LogDataLive/s115.csv\n",
      "copy: s3://mbhavanagarwala-week9/data/LogData/s215.csv to s3://mbhavanagarwala-week9/data/LogDataLive/s215.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://mbhavanagarwala-week9/data/LogData/s115.csv s3://mbhavanagarwala-week9/data/LogDataLive/s115.csv\n",
    "aws s3 cp s3://mbhavanagarwala-week9/data/LogData/s215.csv s3://mbhavanagarwala-week9/data/LogDataLive/s215.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f245ecc8-1a90-4a36-9c2d-c1d25b5a2827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f1a8d13e5d47d3b096419892f343cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|serverID|avgVolume|\n",
      "+--------+---------+\n",
      "|      s2|    920.0|\n",
      "|      s1|    379.4|\n",
      "+--------+---------+"
     ]
    }
   ],
   "source": [
    "# Rerun the same query to show that the sev2 volume report has been updated.\n",
    "# Wait a while.  You should see rows for both s1 and s2\n",
    "spark.sql(\"SELECT * FROM volumeReport\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy the two log files s1610 and s2610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0589690-9a47-49ac-b303-44e6fad7da49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://mbhavanagarwala-week9/data/LogData/s1610.csv to s3://mbhavanagarwala-week9/data/LogDataLive/s1610.csv\n",
      "copy: s3://mbhavanagarwala-week9/data/LogData/s2610.csv to s3://mbhavanagarwala-week9/data/LogDataLive/s2610.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://mbhavanagarwala-week9/data/LogData/s1610.csv s3://mbhavanagarwala-week9/data/LogDataLive/s1610.csv\n",
    "aws s3 cp s3://mbhavanagarwala-week9/data/LogData/s2610.csv s3://mbhavanagarwala-week9/data/LogDataLive/s2610.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4246b60-8de0-4484-b6f3-274eb1ca3ad5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399024443ff34bb9b472f4670b966f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|serverID|avgVolume|\n",
      "+--------+---------+\n",
      "|      s2|    519.9|\n",
      "|      s1|    199.7|\n",
      "+--------+---------+"
     ]
    }
   ],
   "source": [
    "# Run the query again to verify that the report was updated. \n",
    "# Be sure to wait for a little while to make sure the query is updated.\n",
    "spark.sql(\"SELECT * FROM volumeReport\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b8e2417-e879-45bf-a99e-bedb89fd9e81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 2. Get the SEV0 log report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all files from your \"live\" directory before working on this part.  (Note, deleting all the files will make the directory go away.  Don't worry, we'll put it back!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 rm s3://mbhavanagarwala-week9/data/LogDataLive --recursive --include \"*.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the LogDataLive folder is gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE data/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 ls s3://mbhavanagarwala-week9/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "744e269d-0f08-4634-aca5-bd9d4aac8741",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334f55b4358048cfa86cedc731645364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a data frame on top of your original data frame that holds the raw data, \n",
    "# this data frame for the sev0 report is just <serverID> <time stamp> ordered by timestamp, \n",
    "# and by server ID within timestamp\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter SEV0 events and select relevant columns\n",
    "sev0 = streamingLogData.filter(col(\"severity\") == 0) \\\n",
    "    .select(\"serverID\", \"timestamp\") \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e9bb771-f5ba-4295-be8a-061363ab8eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab074567e8944057a2e978831b2f4c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a query on your sev0 data frame that writes the table (data frame) to a csv file in your data/StreamingOutput folder\n",
    "\n",
    "sev0SaveQuery = sev0.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"s3://mbhavanagarwala-week9/data/StreamingOutput/\") \\\n",
    "    .option(\"checkpointLocation\", \"s3://mbhavanagarwala-week9/data/checkpoints/\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Copy two files s115 and s215 into the live folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://mbhavanagarwala-week9/data/LogData/s115.csv to s3://mbhavanagarwala-week9/data/LogDataLive/s115.csv\n",
      "copy: s3://mbhavanagarwala-week9/data/LogData/s215.csv to s3://mbhavanagarwala-week9/data/LogDataLive/s215.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://mbhavanagarwala-week9/data/LogData/s115.csv s3://mbhavanagarwala-week9/data/LogDataLive/s115.csv\n",
    "aws s3 cp s3://mbhavanagarwala-week9/data/LogData/s215.csv s3://mbhavanagarwala-week9/data/LogDataLive/s215.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the contents of the output folder.  Make a note of the non-empty part files generated.  That/those are your SEV 0 report after processing times 1 through 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-01 03:57:25          0 \n",
      "2024-06-01 06:29:42          0 _spark_metadata_$folder$\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 ls s3://mbhavanagarwala-week9/data/StreamingOutput/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the s1610 and s2610 files into the live folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://mbhavanagarwala-week9/data/LogData/s1610.csv to s3://mbhavanagarwala-week9/data/LogDataLive/s1610.csv\n",
      "copy: s3://mbhavanagarwala-week9/data/LogData/s2610.csv to s3://mbhavanagarwala-week9/data/LogDataLive/s2610.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://mbhavanagarwala-week9/data/LogData/s1610.csv s3://mbhavanagarwala-week9/data/LogDataLive/s1610.csv\n",
    "aws s3 cp s3://mbhavanagarwala-week9/data/LogData/s2610.csv s3://mbhavanagarwala-week9/data/LogDataLive/s2610.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the contents of the output folder again.  Make a note of the NEW non-empty part files generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE _spark_metadata/\n",
      "2024-06-01 03:57:25          0 \n",
      "2024-06-01 06:29:42          0 _spark_metadata_$folder$\n",
      "2024-06-01 06:37:12          0 part-00000-7baff181-1bfe-4aae-abe7-6f4c6c1e17e4-c000.csv\n",
      "2024-06-01 06:37:09         15 part-00000-9c376fb5-f84a-4e58-87f4-3d451757ce64-c000.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 ls s3://mbhavanagarwala-week9/data/StreamingOutput/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "edc2cd6e-64f8-4a43-b505-ef7c8b27955e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd8fe51867d4a65aa28f6e57f0a8ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping stream: None\n",
      "Stopping stream: volumeReport\n",
      "All streaming queries stopped."
     ]
    }
   ],
   "source": [
    "# Now you're done with the lab\n",
    "# clean up / stop all running streaming jobs\n",
    "# Stop all active streaming queries\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping stream: {stream.name}\")\n",
    "    stream.stop()\n",
    "\n",
    "# Verify that all streams are stopped\n",
    "print(\"All streaming queries stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put Your Sev 0 Report Here\n",
    "\n",
    "Your Sev 0 report(s) exist as CSV part files on S3.  The first version of the report is the contents of the part files added after adding the log files for times 1 through 5, and the second version of the report is the contents of all the part files (i.e. after adding both sets of log files).  In the next two markdown cells you will copy in the contents of the report.  You will get the lines by downloading the CSV files from S3 and copying their contents into the respective markdown cells, notice your file contents will be wrapped in a ```pre``` tag (preformatted output).\n",
    "\n",
    "**Important Note**: Make sure your notebook can be executed from beginning to end without error. You should check that before you hand it in. Simply putting results into a non-working notebook will not be considered as a valid submission."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contents of the sev 0 report after processing times 1 to 5\n",
    "<pre>\n",
    "s1,5\n",
    "s1,5\n",
    "s1,5\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contents of the sev 0 report after processing times 1 to 5 and 6 through 10\n",
    "<pre>\n",
    "s2,9\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "StreamingLab",
   "notebookOrigID": 3258061014311749,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "name": "StreamingLab",
  "notebookId": 3750749627479206
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
